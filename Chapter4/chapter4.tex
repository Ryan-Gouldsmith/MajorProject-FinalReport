\chapter{Testing}

%Detailed descriptions of every test case are definitely not what is required here. What is important is to show that you adopted a sensible strategy that was, in principle, capable of testing the system adequately even if you did not have the time to test the system fully.%Have you tested your system on �real users�? For example, if your system is supposed to solve a problem for a business, then it would be appropriate to present your approach to involve the users in the testing process and to record the results that you obtained. Depending on the level of detail, it is likely that you would put any detailed results in an appendix.%The following sections indicate some areas you might include. Other sections may be more appropriate to your project.

This chapter discusses the testing strategy which has been implemented on the project. This includes unit, acceptance and user testing utilised throughout various parts of the application.

\section{Overall Approach to Testing}
To recape, an agile approach was adopted throughout the project. From the process, test-driven-development was used throughout the application for almost all aspects of testing.

\subsection{Test-driven-development}
Test-driven-development(TDD) was adopted throughout all aspects of the application. All implementation code as an test which covers the purpose of the implementation. Figure \ref{fig:tdd} shows the TDD cycle.

\begin{figure}
  \includegraphics[scale=0.5]{images/tdd}
  \centering
  \caption{The cycle of TDD during the development stages of the application}
  \label{fig:tdd}
\end{figure}

A sensible test was created and, following the cycle, this would fail when first tested. The following steps would be to ensure that the tests pass by adding the associated code needed to make sure that it passes - afterwards refactoring occurs to ensure that design is kept simple and as clean as possible for the current implementation.

This approach could have been modified so that a group of tests were created for a feature and then implement a set of functions. This testing strategy was rejected and a pure one test for one bit of functionality was used. This was mainly to ensure that the design was not being over complicated.

Reflecting on the testing strategy and the design, it really did help to think of the design through a test-driven-development way. It ensured that the domain was fully understood before creating a test. This left the codebase tested fully - through a variety of aspects.
\section{Automated Testing}
One thing to note is that Flask's testing documentation is very sparse and is of low quality.

During the first few iterations of tests pytest was originally being used to create test classes, making all the classes extent $unittest.TestCase$.

Flask tests were refactored mid-way through the series of sprints to use Flask-testing[Cite]. This offers better testing support for Flask application, allowing the creation of dummy application and providing the functionality to run a quick server for testing.

\section{Mocking Tests}
Mocking during tests is changing the output of a function to return a specific value which is known about [CITE]. It was established that certain tests would need to be mocked, because some data would change from test to test. It was identified that \textit{all} interactions with Google API's, any interactions with Tesseract and the Session would need to be mocked.

It is best practice when developing application that the developers should not hit a production URL during development and testing. As the Google API does not support specific environment API's, then all URLs would be to a production URL. The issue this raises when testing is ensuring that the tests are isolated and pass every time. For example, if the test queried the API one day it will return a specific result, but when queried another day it may return another result; this requires  mocks to be used. The mock would return a specific value every time, ensuring tests can be reliable.

The principle is the same for testing Tesseract in the web application. If more training was conducted then the results from the test on the image would change, therefore ensuring that the tests have consistency data was mocked from specific functions - to check they returned the correct output.

During the first few sprints, whilst understanding how mocks work with Python, there was a lot of duplication with the mocking services. The library mock [CITE] uses annotations above test functions to signal a mock value.

\begin{lstlisting}[language=python, caption={An example of using mocks, following the annotation pattern}, label={lst:mock1}, breaklines, columns=fullflexible, keywordstyle=\color{blue}]
  @mock.object(GoogleOauthService, 'authorise')
  @mock.object(GoogleCalendarService, 'execute_request')
  def test_return_correct_response(self, authorise, calendar_response):
    authorise.return_value = some_json
    calendar_response.return_value = some_more_json
\end{lstlisting}

Mocking example \ref{lst:mock1} shows the syntax which was initially used by the tests. This would result in many of the tests becomming unreadable and easy to follow exactly what the point was. Additionally the do not repeat yourself (DRY) principle was violated, by duplicating much of the codebase.

Looking the mock API documentation, patching object calls was discovered. Implementing this solution reduced the amount of code for mocking specific functions. The annotations were removed from the top of function tests. Initally it was not entirely clear how to implement these patch functions. Eventually the patch was included in the $setUp$ and $tearDown$ functions, as shown in figure \ref{lst:mock2}. The code for mocking is a lot more succinct.

\begin{lstlisting}[language=python, label={lst:mock2}, breaklines, columns=fullflexible, keywordstyle=\color{blue}, label={Mocks using the patch and start. It stops in the dear downs}]
    def setUp(self):
      # some code
      authorise_patch = mock.patch()
      authorise_mock = authorise_patch.start()
      authorise_mock.return_value = some_json


    def tearDown(self):
      mock.patch.stopAll()
\end{lstlisting}

Often when testing the code there needed to be ways in which the output varied depending on when it was called. In Python mock library they had the functionalty for that, and it was called side effects.

\begin{lstlisting}[language=python, label={lst:mock3}, breaklines, columns=fullflexible, keywordstyle=\color{blue}]
    def setUp(self):
      # some code
      self.google_patch = mock.patch.object(GoogleCalendarService, "execute_request")
          self.google_mock = self.google_patch.start()
          self.google_mock.side_effect = [self.google_response, self.new_event, self.google_response, self.updated_response]

    def teatDown(self):
      mock.patch.stopAll()
\end{lstlisting}

Figure \ref{lst:mock3} shows the use of the ``side effect''. It is essentially, a way to define a series of output. From the above exame, it outputs google response first, then when $execute_request$ is called for a second time $new_event$ response is fired and so on. This was implemented due to the way the code was implemented in the controller, where multiple calls to the same function were executed, but different results were needed from the calendar.

Overall, mocking produced a substantial amount of testing. It was not considered when designing the system that mocking would be needed, it was only when testing happened that it was realised it was needed. The tests were refactored to incorporate the mocking facilities.

\subsection{Unit Tests}
After refactoring the testing infrastructure to use Flask-testing fraamework instead of the default python unit test, then unit testing became event easier.

The tests would extend the libraries TestCase class. This allows the application access to the application context (the current application) to test models.

Unit tests were conducted for all the classes in the model directory. Model testing themselves was relatively easy and simple to implement.

\subsection{Route Testing}
As the application was using routes then tests were conducted to ensure that the routes could be hit. Whilst doing TDD these were often the first tests to be written. This allowed design considerations to be thought over regarding specific routes.

Other aspects of testing the routes ensured that the correct response codes were returned, ensuring that the routes were working as intended from the very core of the application. Additional tests were added to check that any redirects worked correctly, to ensure flow between controllers were being executes where applicable.

Finally, in some sections persistance testing occured where a test database was checked to make sure that code inside the controllers was being executed. For example, when adding a note the note route checks to make sure that a note was created in the test database.

\subsection{Handling sessions}
One of the tricker aspects regarding testing was the handling of sessions. In parts of the application sessions are used to handle specific states of the system, i.e user logged in.

During testing of routes then session handling would not be too complex. The flask documentation gives a clear breakdown of how to modify a session. To begin with you have to open a test client context - this mirrors what a normal client would look like. From there a session transaction context needs to be opened. Once this has been opened then it would allow for session modification.

Issues arose during the acceptance tests where session modification would need to happen. Issues arose due to the fact that selenium would run on a different process to the application. As a result session modification could not occur during using selenium and acceptance test. This caused a lot of problems regarding testing. As a result, more mocking had to be used. This time the session helper would be then ended up mocking.

Overall, session handling was one of the hardest aspects with testing to get around and find a suitable, yet clean solution.

\section{Integration Testing}
Integration tests would aid in thinking about not just the backend models, but how the information would be presented to the end user.

These tests were implemented in the form of using Python's selenium tool. As previously used before, selenium is a testing tool which allows you to integrate with the DOM and perform automated browser testing. These tests are important to ensure that the user interface is tested for input and ensuring values are correct.

An example of a selenium test to ensure that when the user goes to the file upload page and they're entering information into the meta-data form that associated calendar events are correctlt displayed.

Another useful test where selenium works well is when checking the background colour of the Tesseract output. By mocking the tesseract output the tests were able to test logic such as identifying the confidence score to the correct class name - and therefor the classname being identified with the correct background colour of the text.
\section{User Testing}
Due to the application aiming to solve a problem with a set of students then user studies were conducted and teh responses were analysed and evaluated. Students partook in a usability study which the aim would be to show where the application was good and where it was not so good.

Prior to the actual scheduled user-testing feedback was given regarding the Tesseract output confidence. This ``over the shoulder'' comments were along the lines of: ``It would be great if you could click the identified text and it would automatically populate the text boxes''. This was then implemented as a result from pre-user testing.

Further issues which were identified during the user testing were:
\begin{itemize}
  \item Uploading a JPG image off a phone, which does not have the correct datetime exif key causes the application to fail.
  \item Uploading an image with a previously uploaded filename caused the application to display the old file.
\end{itemize}

These issues were caught and modified thanks to extensive user-testing of the application.

One interesting reflection of the user-case study was that people would not use the application. They were quick to defend the applications quality, but the use-case for them taking notes was not present. They much preferred to write up their notes from the lecture for memory retention.
\section{Tesseract Testing}
Due to there being no code written for the Tesseract training process there were no formal tests conducted for this section. However, what could be tested was how well the Tesseract learned as it was going through the learning process.

A test framework was produced which ran the training process on the image. As mentioned, it outputs a text file which is associated to the words identified. This file was then renamed to $eng.ryan.expx_stat.txt$ so that when the final part of the training process is ran it did not overwrite the file.

A statistical analysis was collected on the outputted characters from each of the training examples. Each of the characters were identified on each line of the text file, where each character represents a box which Tesseract has identified as a character.

\section{Image thresholding Testing}
When writing my script for the image thresholding research it was soon discovered that the script would be very trial and error and very much like a protoyping.

When testing the output from the image, then it would be a visual check rather than a specific image that was looking to be achieved. Looking at each itteration through the scripts improvement it would be evaluated as whether it was a worse or better binarisation script. Once a sufficient level of satisfaction from eye-judgement was achieved then the testing from that script was stopped.

Once the spike work for the script had been completed, TDD performed on the image to turn the prototyped script into clean testable code. Using the outputted numpy arrays from open cv, they were compared against what was believed to be outputted.

When the image converts the white image mask to black it performs checks for the image for any black value in the image.
