\chapter{Implementation}

%The implementation should look at any issues you encountered as you tried to implement your design. During the work, you might have found that elements of your design were unnecessary or overly complex; perhaps third party libraries were available that simplified some of the functions that you intended to implement. If things were easier in some areas, then how did you adapt your project to take account of your findings?%It is more likely that things were more complex than you first thought. In particular, were there any problems or difficulties that you found during implementation that you had to address? Did such problems simply delay you or were they more significant?%You can conclude this section by reviewing the end of the implementation stage against the planned requirements.\section{Image processing}\subsection{Optimising Tesseract}After the design considerations to use OpenCV was chosen, specific algorithms would need to be implemented. After a discussion with Dr Hannah Dee, it was suggested that investigations into different binarisation scripts would be useful to see how an image can be converted to black and white from an image.\subsubsection{OTSU}[CITE CITE, Lab brooks page]OTSU, created by Nobuyuki Otsu, is a binarisation technique which allows the conversion of an image to black and white. Otsu is a global thresholding algorithms, where it takes all the image and compares that, rather than local thresholding, where it's pixel by pixel. [CITE Automatic thresholding for defect detection].OTSU requires uniform lighting conditions to be fully effective, this is naturally a problem when taking notes due to shadows being placed over the note.With OTSU, the image is seperated into a histogram of grey-level values. OTSU determines the optiminal threshold value from the histogram, by ``maximising the discriminant measure''[CITE OTSU PAPTER][CITE OPENCV]. %TODO maybe word this better?Once the threshold has been established, it will serparate them into the foreground and background pixels. [CITE LAB BROOKS PAGE].[CITE HP PAPER]Tesseract uses OTSU as its preprocessing method for the tiff files presented to the system. From analysis and experimentation using the OTSU method, it was clear to see how exactly the image was failing with simple just a grayscale value. Fig X, shows how Tesseract would intepret the image, making it almost impossible to see characters on the page.\subsubsection{Adaptive Threshold}From the enlighting analysis of OTSU it was realised that an OTSU thresholding ontop of an OTSU threshold from the Tesseract engine would not be beneficial. To account for the changing of light source across an image, an adaptive threshold was chosen [CITE somewhere which says this].Adaptive threshold works by doing local thresholding across the image, rather than the global thresholding OTSU does. Therefore, there is more chance that shadows on an image would be eliminated from the local thresholding.During the trials of optimising the images for Tesseract, other thresholds were tested: Fig x shows the binarisation and thresholding algorithms implemented on a note.From Fig x, it's unoquivacal that the adpative threshold using mean and gaussian returns the best binarisation success rate. The text on the image is legiable, and it produces an image which clearly segments the text from the background; the mean has more noise on the image, but these would be cleared up with morphological events.As a result the adaptive threshold function with Gaussian blur was chosen to be the thresholding used for the pre-processing.\section{Lined paper}Initially, standard lined paper was used; the noise that was produce was far too much to make Tesseract analyse characters correctly.Due to Tesseract requiring text to be in horizontal lines [CITE HP PAPER] plain paper would often skew the text and make it harder for Tesseract to interpret the text.\subsection{Filtering the blue lines}As a result, custom blue lined paper Fig X, gives equal spacing between the lines as well as a horizontal structure. Blue was initially chosen as the pre-processing script would filter the blue lines and leave the black, binarised, text. The text would then appear as though it was written on normal paper and should not be included with the noise from the image.This process went through a series of iterations to try and overcome issues identified through each iteration. The first process was to extract all the values on the page which fell between a predefined grey-black range. However, this had its obvious problems that it would extract some of the line where a dark blue would contain black-like elements of colour.Morphological operations such as erroding [CITE] and dilation [CITE] was performed on the image, but to no avail - there was still noice in the image which causing Tesseract to interpret them incorrectly. This also had an unwanted side-effect: the binarised characters would then become so undistinguishable that they would not be able to be located correctly by the OCR tool. They became so pixelated that even from a human eye it was hard to identify their meaning.\subsection{Only extracting the text}The following optimisation would identify the lines, but only extract the text that was needed.[CITE EXAMPLE ON OPEN CV]A gaussian adaptive threshold operation was performed over a median blurred, grayscale version of the note; successfully binarising both the text and the lines. The horizontal lines on the page were extracted using OpenCV's structuring element, $MORPH\_RECT$ [CITE].Further errosion and dilation was applied to remove the black lines and any noise residue from the extraction. A series of intermediate blank image masks were created to transfer the text from the image across to the mask. Although this carried all the text over, further line noise was also transferred across.This suffered similar problems are the previous iteration, until connected components, via contours was discovered. Due to the morphological operations on the horizontal lines the connectivity between the pixels was very small. Thereby, choosing to use connected components would be able to extract the text from the image with little interference from the horizontal line noise. These connected components were then transferred to the new mask - only carrying the text from the image, not the lines.Final morphological operations were included to improve the image quality. Overall the binarisation script works well.  It can take a photo of an image in a terrible light source Fig x, for example, and it would succesfully output the binarised image which would have distinguishable text, coinciding with little noise on the image. The noise on the image has little interference with the Tesseract OCR and it yields better results than greyscale images.\section{Handwriting Training}During the start of the handwriting training phase problems occurred such as it not reading the characters from a greyscale image correctly.\subsection{Training process}When using training data to be worked with Tesseract, it requires it to be in a specific file format. Such as: $<lang>.<font>.exp<number>.tiff$. When ran through a language, Tesseract outputs a box file which contains on each line: the character and the coordinates of this box. Trying to analyse this box was almost impossible. On the Tesseract wiki page [CITE] there's a link to jTessBoxEditor [CITE]. This tool was used to tag the boxes with their associated content.Whilst using this editor the boxes could be expanded or shrunk to give the best possible fit to the characters. This was often utilised due to erroneous characters picked up by the editor.% add some more stuff about training here.\section{Web application}\subsection{OAuth}\subsection{Reoccuring events}Reocurring events were discovered as an issue in the pre-beta user testing. During the design phase when thinking about the calendar, it was forgotten that reocurring events and all-day events could exist.All-day events do not have the dateTime key response from the Google Calendar API. As a result the code would fail when trying to access the dateTime from the event start date. This resulted in a redesign and re-think of the possible issues which could arise from Google Calendar.Eventually, the dateTime key was checked and the all day events issue was solved.However, the reocurring events problem was still existing. When querying for an event, if the event was reocurring then it would group the reocurring events by the first time in which the event was created. This resulted in an image, which was taken on the 12th March 2016 for example, to show events in February - if there was a reocurring event on the 12th March. However, it had an important reoccurence event ID key.This resulted in a further query being created which would return all the instances that were reocurring. This had to pass in the $event['id']$ to the query to return all these instances; it was filtered down by querying for the start and end date.When editing a reocurring event, Google calendar performs some unexpected behaviour: instead of silently modifying the event and returning the grouped event, again, it instead returns both the grouped event and the edited event. A succinct solution for this has not been found and has been a slight issue.\subsection{Tesseract Confidence}During a meeting with Dr Hannah Dee, it was suggested that some form of confidence score couldbe outputted to the user to show how well Tesseract identifies the text from the image.The Tesseract command line does not output the confidence of the characters identified; only the C++ library can output the confidence. Due to time constraints, a wrapper for the C++ Tesseract API could not be implemented - so a third party library was chosen, tessocr[CITE].Tessocr offered the implementation to access the confidence values for the associated words. The algorithm to execute the identification of the characters was quite simple.Firstly we get all the text lines; Tesseract deals with the lines as a series of text lines. This is then enumerated over and for each line a corresponding list of confidence words is collected from tehe $map_all_words$ API.Due to this returning a tuple which is an immutable object in Python, modifications on the list could not be made easily. This resulted in the view file checking the tuple content and calculating whether it is above the threshold; 75 for green, 70 for orange and below 65 for red.\subsection{Displaying calendar events}\subsection{Parsing Exif data}\subsection{Editing calendar events}\section{Review against the requirements}