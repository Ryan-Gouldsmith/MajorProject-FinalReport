\chapter{Implementation}

%The implementation should look at any issues you encountered as you tried to implement your design. During the work, you might have found that elements of your design were unnecessary or overly complex; perhaps third party libraries were available that simplified some of the functions that you intended to implement. If things were easier in some areas, then how did you adapt your project to take account of your findings?%It is more likely that things were more complex than you first thought. In particular, were there any problems or difficulties that you found during implementation that you had to address? Did such problems simply delay you or were they more significant?%You can conclude this section by reviewing the end of the implementation stage against the planned requirements.\section{Image processing} \label{imp:image_proces}Image processing would prove to be an integral part of the application. There was a need to correctly convert the image from a photo representation to an image which Tesseract could identify. The binarisation script went through many iterations prior to getting the output as shown throughout this section.\subsection{Optimising tesseract}Prior to the OpenCV being used for the tool for binarising. Several attempts were made to binarise an image using ImageMagick. In sprint zero, converting the image to greyscale was attempted but this returned poor results from the Tesseract. The next iteration of the script was to use monochrome on the image, this turned the images to black and white, but left a lot of additional noise. After struggling to find how to use morphological operations it was suggested by Dr Hannah Dee, to investigate binarisation with OpenCV.\subsubsection{OTSU}OTSU, created by Nobuyuki Otsu, is a binarisation technique which essentially converts an image to black and white. Otsu is a global thresholding algorithm, using the whole image to compare the pixel values. This is unlike local thresholding algorithms where comparisons are made pixel by pixel \cite{citeulike:6044081}.Due to notes having non-uniform lighting, due to notes often having shadows on the image, this makes OTSU problematic when binarising images which were taken from an outdoor environment.\begin{figure}[H]  \centering  \includegraphics{images/OTSU}  \label{fig:OTSU}  \caption{The use of OTSU binarisation technique on an image with a little shadow across the image}\end{figure}Figure \ref{fig:OTSU} shows the OTSU binarisation method used on an image with the a slight shadow over the top right of the image. It can be clearly seen that the binarisation segments the image into two distinct regions: the bottom half is white whereas the top left is black. Due to this being the actions of global thresholding it can be clearly seen that it is not ideal when trying to perform clear binarisation yet, the characters can not be identified.The basic premise of OTSU is to segment the grey-level from the image into a series of histograms. OTSU then determines the optimial threshold value by ``maximising the discriminant measure'' \cite{citeulike:2917492}. Essentially, OTSU attempts to maximise the margin between the histograms, this margin would then act at the threshold value as to whether a pixel is segmented as either a foreground or background pixel \cite{citeulike:14021372}.HP, the creator of Tesseract, describe OTSU as its underlying pre-processing algorithm when converting the image prior to extracting textlines and characters \cite{citeulike:13931186}. Once the spike work was completed with OTSU, Figure \ref{fig:OTSU}, it was clear to identify that Tesseract would find it difficult to identify the characters from the image when the output was so damaged.Overall OTSU, although it is a very solid binarisation method, suffers from imposed shadows over images. This would not be the best option to choose when choosing an appropriate binarisation technique as more often than note, the photo would be taken in non-uniform lighting.\subsubsection{Adaptive threshold} \label{section:threshold}The enlighting analysis of the OTSU binarisation technique discovered that using an OTSU for the pre-processing step for Tesseract would not be beneficial. Passing an OTSU binarised image to the Tesseract engine which performs OTSU on the image again would not be beneficial and would not aid in improving the accuracy of the Tesseract engine. As a result, the next iteration evaluates an adaptive threshold technique.Adaptive threshold calculates the threshold over a series of smaller segments in the image \cite{citeulike:14021401}. This means that shadows have a smaller impact over the whole image, due to adaptive threshold being a local threshold technique. By not considering global thresholding, the image become more invariant to shadows.Using the OpenCV library there was two options with adative threshold \cite{citeulike:1402140}:\begin{enumerate}  \item Gaussian adaptive threshold: the weighted sum of the neighborhood  \item Mean adaptive threshold: the mean of the neighborhood.\end{enumerate}\begin{figure}[H]  \centering  \includegraphics{images/adaptive_mean}  \caption{Adaptive mean threshold algorithm on a note, showing binarisation but there is still noise in the image.}  \label{fig:adaptive_mean}\end{figure}Mean adaptive thresold uses a specific block size around a given pixel. If the block size was four, then the neighbourhood size would be four and calculations would be made to calculate the mean pixel value in that block. This is then selected as the thresholding value for the pixels inside the block and will determine whether it falls into the foreground or background \cite{citeulike:14021401}. Figure \ref{fig:adaptive_mean} shows a mean adaptive threshold. An additional issue present with the adaptive mean is the noise pixels; the image shows the noise as small black pixels.Gaussian adaptive threshold differs from the Mean adaptive threshold as instead of calculating the mean value over the block size, it first uses a Gaussian value over block. A Gaussian weight is calculated depending on the blocksize entered. Every pixel inside the block is then multiplied by the Gaussian, and an average value is then taken across the pixels and is used as a threshold. Like Mean adaptive threshold, this is then used to determine if the pixel is foreground or background \cite{bradski2008learning}\cite{citeulike:1402140}.\begin{figure}[H]  \centering  \includegraphics{images/adaptive_gaussian}  \caption{Adaptive Gaussian used over the image, showing a lot smoother of an image}  \label{fig:adaptive_gaussian}\end{figure}Figure \ref{fig:adaptive_gaussian} show the adaptive Gaussian being used to binarise an image. The output clearly does not have a shadow overlaying the image. This shows a clearly binarised image with little noise on the image.\begin{figure}[H]  \centering  \includegraphics[scale=0.3]{images/thresholding_options}  \caption{A variety of thresholding techniques used on the same note, showing adaptive threshold resulting in the best}  \label{fig:thresholding_options}\end{figure}Figure \ref{fig:threshold_options} displays additional types of the tresholding that were investigated throughout the iterations of investigation. The Gaussian adaptive threshold provides the clearest results from visual inspection of the different thresholding techniques tried. As a result, the the Gaussain adaptive threshold would be continued to be improved upon throughout a series of iterations to reduce additional noise, via morphological operations to aid smoothing.\section{Lined paper}Initially normal lined paper was used for the note taking, but after the binarisation it left too much noise. Further smoothing of the image did not remove the noise, so bespoke lined paper was created to aid in removing the lined but keeping the uniformly straight text. Refer to appendix \cite{appendix:image_processing} section \cite{processing:pre-line}.\subsection{Filtering the blue lines}Over a series of iterations, the primary objective was to remove the blue lines from the image. Examples of the lines paper can be found in Section \ref{}.\begin{algorithm}\caption{Initial removing the blue lines algorithm}\label{algorithm:threshold1}\begin{algorithmic}[1]  \Function{remove\_lines}{}    \State image $\gets$ read\_image\_as\_grayscale()    \State lower\_black $\gets$ np.array([0,0,0])    \State upper\_black $\gets$ np.array([175,20, 95])    \State mask\_black $\gets$ cv2.inRange(erode, lower\_black, upper\_black)    \State mask[np.where(mask\_black == 0)] $\gets$ 255  \EndFunction  \end{algorithmic}\end{algorithm}Algorithm \ref{algorithm:threshold1} attempted to filter out all the colour values between a grey and a black range. By restricting it to this range intended to remove the blue lines from the image. However, the blue lines would still contact dark pixels - so only a selection of the lines would be removed.\begin{figure}[H]  \centering  \includegraphics{images/removed_lines_still_noise}  \caption{An example output from the above algorithm. There is still significant amounts of noise in the image.}  \label{fig:remove_lines_noise}\end{figure}Different morphological operations were used on the image in an attempt to clear the noise pixels. Errosion operations were used, by passing a kernel over the image essentially removing small black noise pixels against a vast white background whilst making the blacker pixels larger, enhancing the text on the page [CITE OPEN CV]. Dilation is essentially the opposite: a kernel is passed over the image, and the white background areas get larger and the black text gets thinner; this has the effect of removing the characters quality [CITE OPENCV].The result of the morphological operations ened up reducing the quality of the segementation, as shown in Figure \ref{fig:remove_lines_noise}. This highlighted further iterations were needed for an improved output.\subsection{Only extracting the text}There was no simplistic way to identify and filter the lines, therefore it was decided that the text will just be extracted.OpenCV has an example of line extraction and binarsation \cite{citeulike:14006256}. From the example, structuring elements were used to extract the text from the image. Further errosion and dilation were used to remove additional noise. Throughout the process, masks were used to transfer the state of the image to another mask. An example is, transferring the text to a mask, but the line noise was transferred too.Due to the text having connected pixels, unlike noise, then connected components were used to identify characters. Due to morphological operations the lines were no longer connected. The identified characters were copied to a final mask.Further morphological operations cleared up the image. After a series of iterations the binarisation technique had been completed.\begin{figure}[H]  \centering  \includegraphics[scale=0.3]{images/hard_image}  \caption{A poor quality image has been binarised successfully with little noise.}  \label{fig:poor_quality}\end{figure}Figure \ref{fig:poor_quality} shows the result of the binarisation script. Adaptive threshold works well over the image, due to local thresholding not being affected by shadows. Images can be taken in non-uniform lighting and it will produce a fully binarised image. There were issues which affected the implementation such as changing to a bespoke lined paper. Overall the binarisation segmentation works well. Further examples can be found in Appendix \ref{appendix:image_processing}.\section{Handwriting training}Training the handwriting was a constant task through out the sprints. It was intially proving cumbersome in the early sprints. After the changes implemented from Section \ref{section:threshold} the results from the handwriting recognition improved considerably.\subsection{Training process}Tesseract's training was a methodical process. Tesseract's GitHub wiki and That guy's thesis, provided great reference tools on how to train the data.{{\ttfamily \hyphenchar\the\font=`\-}%Firstly, as handwriting was being trained a new language would have to be created. Each training has a specific format which must be adhered to: \texttt{lang.font.expNumber.tiff}. The file is then run through the Tesseract training process using \texttt{batch.nochop makebox} command, on the specific language \texttt{eng.ryan.exp2a}, to create box files from previous examples. The box file contains the characters which Tesseract believes are in the image; each line is a new character.{{\ttfamily \hyphenchar\the\font=`\-}%The box files were complex to analysis as it was not intuitive to see the identified characters without a graphical interface. Figure \ref{fig:box_editor} shows the use of the jTessBoxEditor \cite{citeulike:13926798} tool to identify characters and their bounding boxes to overcome this issue found.\begin{figure}[H]  \centering  \includegraphics[width=\textwidth]{images/box_editor}  \caption{A example of the jTessBoxEditor being used identify characters in the tiff box file.}  \label{fig:box_editor}\end{figure}{{\ttfamily \hyphenchar\the\font=`\-}%Once the characters had been manually changed, the box file was passed through Tesseract's training process. \texttt{tesseract <file> -l eng.ryan.exp2a box.train} would train the engine on the image's box file, for the language \texttt{eng.ryan.exp2a}. Often there were issues with being unable to identify tagged characters; these box file lines were deleted.{{\ttfamily \hyphenchar\the\font=`\-}%Following this process, Tesseract required an additional file, \texttt{unicharset\_extractor}, to be able to extract all possible characters that is identified in the training examples.{{\ttfamily \hyphenchar\the\font=`\-}%Tesseract's GitHub repository states that clustering is an important process to extract ``prototypes''. The clustering commands, aid in ensuring the shapes of the characters are known.A frequent words file was created, from the \texttt{/usr/share/dict/words} directory, to help to identify common words. This would aid in improving the changes of detecting these words. Common words could also be defined; ``by'' and ``date'' are examples of words in this file.{{\ttfamily \hyphenchar\the\font=`\-}%Finally, the \texttt{combine\_data} command was used to combine all the data together and output a trained data file in the form of ``eng.ryan.exp2a.trainedddata''. This was then copied to the shared Tesseract data directory enabling new training data to use this language.As this was an iterative process, reinforcement of the learning was needed. This was called ``bootstrapping''. Therefore when training on another example, if the language was set to \texttt{eng.ryan.exp2a} it would reinforce that specific language with new data.Throughout the sprints issues were identified whilst training the data. Characters would often be not identified correctly on the image, with specific issues with the letter ``g''. When a blob could not be identified at all, Tesseract would label it ``\~''; these were ultametely removed when it was discovered that it would fail to identify them if edited. The training was conducted on twelve training examples, a selection can be found in Appendix \ref{}.\section{Web application}The web application was the main part of the development and specific sections proved to more complex than anticipated.\subsection{OAuth}\label{app:oauth}The Google Oauth integration was more complex than first considered. Google suggested to use the Google client library for all oAuth requests, for better security with the application. Therefore, this library was utilised throughout the project.Google API client would, on occasion, raise peculiar errors. Whilst making a query to the calendar API it would raise the error, ``rootURL'', when using the build API call. This was mystifying as it was previously working the day before.  An issue was raised on the library's GitHub repository \cite{citeulike:14021433}. To confuse matters more, when querying the People plus API, it would work perfectly fine - however the Google calendar API would raise an exception. It eventually stopped throwing an exception, but the reason is unknown.The Oauth2 [CITE] was implemented to the application so when the user clicks ``authorise with Google'', it will initialise an OAuth creation.Once the user accepts the application can use specific details then a secure credentials file is returned; these are appended to the user's session.The credentials contain an expiration time, as shown in Appendix \ref{}. If when making a request this expiration time was exceeded, then an error would be raised when querying the API's and displayed to the user. This was fixed by checking to see if the token in the session was still valid, if it was not then it would redirect the user to the log out route, clearing the session and making the user to re-authenticate to refresh the token.\subsection{Reoccuring events}Reoccuring events with the Google Calendar integration, poised a lot of issues. It was identified in the pre-beta testing that if the user has a reoccurring event then it would not append the URL of the note to the event.When a query was made for a list of events, if there were reoccurring events then it would group the events by the first of these events. This proved to be problematic as it would display to the user that the note was taken on the 12th March, for example, but there were events from February being shown.To account for this, the structure of the response was analysed and it was identified that grouped events had a ``reoccurrenceID''key. After finding that the calendar API can fetch reoccurring items, a query was made using this ID key - filtering by the start and end date from the initial query. A check was included to ensure that when displaying to the user the event did not contain the ``reoccurrenceID''key ensuring the singular events were displayed to the user.Editing a reoccurring event produced further unexpected behaviour: when the event had been successfully modified and the calendar URL had been appended, it returned both the grouped event and the modified editied event in the initial query. Google must classify that changed reoccurring events are new instances. Instead of displaying more duplicated events to the user, a check for the \texttt{recurringEventId} key, which was present in the modified event, was conducted; if they was present the event was omitted.Another issue identified were all-day events. All-day events do not have a datetime key in their start response field, returned from the Google Calendar API. This would cause the application to raise an exception and prevent the user from adding a note. A check was made to make sure that the datetime key was present.\subsection{Tesseract confidence}Displaying the Tesseract data went through a series of iterations over the latter sprints.At a basic level intergrating Tesseract into the web application was farely simple and was implemented around sprint eight. The binarisation script, see Section \ref{}, was incorporated to the application. This was added to the file upload route, as the user's image needs to be binarised when it is uploaded. A Tesseract wrapper could have been implemented but due to time constraints a 3rd party library, tesserocr \cite{citeulike:14021437}, was used.\begin{figure}[H]  \centering  \includegraphics{images/tesseract_first}  \label{fig:tesseract_output}  \caption{Tesseract being integrated into the application at a very basic level}\end{figure}Tessocr is a C++ wrapper for the API that Tesseract provides. Tessocr uses textlines to extract the text from the uploaded image. The first three lines were then iterated over, identifying the box for the lines so that text could be extracted. Each of these lines mapped the words identified and the confidence score for each word on a scale of 0 - 100 (0 being uncertain and 100 being certain).The module code, lecturer, location and date were extracted via list-comprehensions, matching metadata structure defined in Section \ref{}. Modifications to the confidence scores were attempted in the controller to replace with class name for colours - so that the view file could render the content easily. Problems entailed when the API returned tuples, an immutable type, so modification was not as eloquent as envisiged, so raw numbers remained.Conditional checks were made in the view files on the confidence score; 75 would be green text, less than 74 and greater than 70 would be orange text and below 65 would be red text. Figure \ref{fig:tesseract_colour} shows the resulting output from the conditional checks. There are anomolies, with ``Date'' for example, it is orange but infact it is extracted perfectly.\begin{figure}[H]  \centering  \includegraphics{images/tesseract_colour}  \label{fig:tesseract_colour}  \caption{Coloured representation of the confidence of the words from the handwriting.}\end{figure}\subsection{Parsing EXIF data}EXIF data parsing would be an important section of the application. EXIF data is essentially metadata about an image [CITE]. When a user uploads a note, it analyses the image for the exif metadata, it uses the date of the image taken to query Google Calendar for additional events.Throughout the sprints the EXIF data parsing was extended to allow for a greater variation in images uploaded. Python's image library [CITE] was used to parse the data. Further additional checks were made to ensure that the images were either JPEG or Tiff as they only contain the metadata.During user-testing issues arose where a participant could not upload their note successfully. The image was formatted a JPEG but the mobile phone photo did not contain the ``dateTime'' EXIF key. Checks were implemented to ensure that this key existed.\subsection{Displaying calendar events}Over several user stories displaying different events around the application was created. The first instance of displaying the events were incorporated into the homepage, displaying the last seven days worth of events from the user. This was simple to implement but provided a strong foothold into the interactions with the Google Calendar API; this stretched from the application to the testing infrastructure.One issue which arose was the change in timezones to british summer time during the project. If there was an event starting at 14:00, and the user queried using  14:00 as the date, then it would not return any results from the Google Calendar. Upon closer inspection if the query was for 13:00 then it would return the correct event. This issue rendered the application to a halt whilst this could be fixed: eventually, the timezone was appended to the user's input and querying with the timezone included returned the correct event from Google.\subsection{Editing calendar events}The user story for adding a note was implemented and as part of the tasks for this story, the note URL must be saved in the calendar event. When the user enters the date into the form, a query would be made to the Google Calendar API to return all associated events from that given day. Checks were then performed to ensure that the module code and the summary of the event matched.This poised the problem of being able to add the note's URL to the correct event; if there was more than one event with the same module code that day then there could be confusion as to which event to add to.Over the next iteration of development, the calendar events were additionally validated against the start date from the event and the user's input.One issues which arose when addtion the note's URL to the description field of the calendar was that it would replace the original content inside the description. This is a major concern for the user's of the application as it would overwrite any data. Another issue it created was that multiple notes could not be attached to a given event. This was changed so that it would append to the description field, rather than overwriting it.The algorithm for additng to a calendar is stated below.\begin{algorithm}  \caption{Adding a note URL to the calendar}  \label{algorithm:threshold1}  \begin{algorithmic}[1]      \State Create a calendar service object      \State Prepare url from saved note      \State Build the HTTP request      \State Find an event for that day from the start time as given      \State Parse the events      \State Check to see if the summary contains module code AND the start date time matches      \If{contains module code}        \State check the response to see if description includes url        \State Save URL to the notes attribute in the database      \EndIf  \end{algorithmic}\end{algorithm}\begin{figure}[H]  \centering  \includegraphics[scale=0.4]{images/saved_to_calendar}  \label{fig:saved_to_calendar}  \caption{Saving a note correctly to a calendar event item}\end{figure}Figure \ref{fig:saved_to_calendar} shows the output from saving a note to a specific date and appending the note's URL to the description field.\subsubsection{Editing a note}Editing a note user story was established midway through the project, and the tasks included that editing a note's start date should be reflected in the user's calendar. If the user changes their date then it should remove from the old calendar item and append to a new one, if it the event exists. This proved to be a complicated implementation.When a user edits a note it would query the API to return the event for the given note, based on the time start which was persisted for the note's relation. This event was then modified by replacing the description field with an empty string, replacing all the content. This caused issues regarding the data stored by the user being deleted arbitrarily. As a result a find and replace was used on the description field to remove any strings which matched the URL.It is worth acknowledging at this point in the development considerable aspects of the codebase was refactored. There was duplicated functionality spread across multiple blueprints, making the codebase obfuscated. As a result the design decision for the \texttt{GoogleServicesHelper} emerged to abstract the duplication.\subsubsection{Logging in and out}The user implementation was an emerging user story midway through the project. As the log in would be defered to Google, then the user would need to connect to the service. When the log in process has been completed a user's email address is extracted from the Google Plus API response and created in the database.Using the application it was noticed that multiple user's wre being created for a single email address. To reduce this problem a helper function was implemented which would find a user from their email address, if the user could not be found then they were added to the relation.The user's ID is then appended to the session. Every page on the application verifies that this key exists. Once a user had finished using the application, the logout route would destroy the user's session.Delegating the responsibility to Google was a good decision in the end,  for security concerns aswell Google have a robust log in system. Futhermore, ethically, the only person information the system uses from a user is their email address.\section{Travis}Although not strightly a coding implementation, Travis formed a core part of the application. Issues were found whilst using Travis, however.Firstly, at the start of the process extensive time was invested into trying to  auto-deploy from Travis. Whilst giving detailed instructions on how to deploy to 3rd party systems the documentation for deployment to a VPS (virtual private server) were sparse. Over several sprints the auto-deploy pipeline was desired but due to the lack of documentation there was no auto-deployment from Travis to a server.When integrating Tesseract with the application, it became apparent of another issue with Travis: Tesseract and OpenCV had to be both built from source. TesserOCR uses Tesseract 3.04, and at the time of writing, Ubuntu's latest package is 3.02; this is the same for OpenCV 3.0.0. As a result, the build time for Travis has increased exponentially. The current build time is around thirty minutes; caching was investigated but no suitable solution has been found.